name: master

on:
  push:
    branches: 
      - master
  pull_request:
    branches: 
      - master

jobs:
  python:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.5, 3.6, 3.7]
        include:
          - python-version: 3.5
            spark-version: 2.3.4
            pandas-version: 0.23.4
            pyarrow-version: 0.10.0
          - python-version: 3.6
            spark-version: 2.4.4
            pandas-version: 0.24.2
            pyarrow-version: 0.13.0
          - python-version: 3.7
            spark-version: 2.4.4
            pandas-version: 0.25.3
            pyarrow-version: 0.14.1
    env:
      PYTHON_VERSION: ${{ matrix.python-version }}
      SPARK_VERSION: ${{ matrix.spark-version }}
      PANDAS_VERSION: ${{ matrix.pandas-version }}
      PYARROW_VERSION: ${{ matrix.pyarrow-version }}
      PYTHON_EXECUTABLE: xvfb-run python
      KOALAS_USAGE_LOGGER: databricks.koalas.usage_logging.usage_logger
    steps:
    - uses: actions/checkout@v2
    - uses: actions/setup-java@v1
      with:
        java-version: 1.8
    - name: Using cache
      uses: actions/cache@v1
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements-dev.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    - if: matrix.python-version == 3.5
      name: Setup Python
      run: |
        sudo add-apt-repository ppa:deadsnakes/ppa
        sudo apt-get install tk-dev python3.5-tk python3.5
        sudo rm /usr/bin/python
        sudo ln -s /usr/bin/python3.5 /usr/bin/python
        # Add the path which packages are actually installed to `PATH`
        echo "::add-path::/home/runner/.local/bin"
    - name: Download dependencies
      run: ./dev/download_travis_dependencies.sh
    - if: matrix.python-version == 3.5
      name: Install dependencies & run tests
      run: |
        sudo apt-get install xclip
        python -m pip install --upgrade pip
        python -m pip install -r requirements-dev.txt
        python -m pip install pandas==$PANDAS_VERSION pyarrow==$PYARROW_VERSION
        python -m pip list
        export SPARK_HOME="$HOME/.cache/spark-versions/spark-$SPARK_VERSION-bin-hadoop2.7"
        ./dev/lint-python
        ./dev/pytest
    - if: matrix.python-version != 3.5
      name: Install dependencies & run tests
      run: |
        curl -s https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh > miniconda.sh
        bash miniconda.sh -b -p $HOME/miniconda
        . $HOME/miniconda/etc/profile.d/conda.sh
        hash -r
        conda config --set always_yes yes --set changeps1 no
        conda update -q conda
        # Useful for debugging any issues with conda
        conda info -a
        # Replace dep1 dep2 ... with your dependencies
        conda create -c conda-forge -q -n test-environment python=3.6
        conda activate test-environment
        conda install -c conda-forge --yes codecov
        conda config --env --add pinned_packages python=$PYTHON_VERSION
        conda config --env --add pinned_packages pandas==$PANDAS_VERSION
        conda config --env --add pinned_packages pyarrow==$PYARROW_VERSION
        conda install -c conda-forge --yes pandas==$PANDAS_VERSION pyarrow==$PYARROW_VERSION
        conda install -c conda-forge --yes --freeze-installed --file requirements-dev.txt
        conda list
        export SPARK_HOME="$HOME/.cache/spark-versions/spark-$SPARK_VERSION-bin-hadoop2.7"
        ./dev/lint-python
        ./dev/pytest
    - name: Update Codecov
      run: bash <(curl -s https://codecov.io/bash)
